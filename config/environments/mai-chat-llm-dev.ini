[SSL]
use_https = False
ssl_keyfile = c:/Projects/01.Python/MAI-CHAT-PROD/bot-controller/config/ssl/key.pem
ssl_certfile = c:/Projects/01.Python/MAI-CHAT-PROD/bot-controller/config/ssl/cert.pem

# Logging Configuration
[LOG]
# Logging level (e.g., DEBUG, INFO, WARNING)
log_level = DEBUG
uvicorn_log_level = WARNING
# Include backtrace in logs for debugging
log_backtrace = True
# Include diagnostic information in logs
log_diagnose = True
# Number of days to retain log files
log_retention_days = 30
# Path to log directory
log_path = ./logs
# Log file name
log_filename = MAIChat_LLM.log
uvicorn_log_filename = uvicorn.log
uvicorn_access_log_filename = uvicorn_access.log
uvicorn_error_log_filename = uvicorn_error.log

# Service Configuration
[SERVICE]
version = 1.8
# Service port
port = 8000
# Enable hot-reloading (development only)
reload = False
# Enable debug mode for detailed error output
debug_mode = False
workers = 4
# Allowed origins for CORS (default: all)
allowed_origins = [*]

[HTTP]
timeout = 60

[SECURITY]
access_password = AI-success!
session_secret = mai-chat-number1!
  trusted_hosts:
    - "localhost"
    - "127.0.0.1"
rate_limit: 100
rate_limit_window: 60

[LLM_MODEL]
llm_model_type=OLLAMA
llm_model_path=C:/Projects/01.Python/MAI-CHAT-PROD/llm-wrapper/models/10.8b-korean-instruct-q8-v1/10.8b-korean-instruct-q8-v1.gguf
llm_model_verbose=True
#llm_model_n_gpu_layers=36
llm_model_n_gpu_layers=-1
llm_model_n_ctx=25000
#llm_model_n_ctx=512
llm_model_n_threads=30
llm_model_n_batch=20
llm_model_max_tokens=1024
#llm_model_max_tokens=512
llm_model_temperature=0.0
llm_model_top_p=0.9
llm_model_streaming=False
llm_model_f16_kv=True
llm_model_repeat_penalty=1.1
llm_model_mirostat_mode=2

[LLM]
llm_backend = vllm
steaming_enabled = False

[VLLM]
endpoint_url = http://10.50.1.43:8010/generate_full

# OLLAMA Model Access Configuration
[OLLAMA]
# Access type for the OLLAMA model (e.g., URL)
access_type = URL
# Model name
model_name = aicess-mistral-nemo-korean:12b
# OLLAMA service URL
ollama_url = http://10.50.1.43:11434
temperature = 0.0
mirostat=2

# API Configuration
[API]
# Retrieval API URL
retrival_api = http://10.50.1.26:5010/v1/retriever

[CIRCUIT_BREAKER]
failure_threshold = 3
recovery_timeout = 60
reset_timeout = 600

# Retriever Configuration
[RETRIEVER]
# Enable filtering in retrieval
filter_flag = False

# Query Filter Configuration
[QUERY_FILTER]
# Enable query filtering
enabled = True
# Handle Korean jamo decomposition
ko_jamo = True
# Support Arabic numerals
arabia_num = True
# Enable wildcard characters in queries
wild_char = True

# Language Model Check Configuration
[LM_CHECK]
query_dict_config_path = C:/Projects/01.Python/MAI-CHAT-PROD/llm-ollama-wrapper/config/prompts/query_check_dict.json
# Supported language keys
query_lang_key = ko,en,jp,cn
# Query categories
query_dict_key = greetings,endings,farewells

# Prompt Configuration
[PROMPT]
# FAQ categories
faq_type = komico_faq_it,komico_faq_lab
# Use source priority in answers
source_priority = False
# Primary source type
source_type = mico_hr_it
# Excluded source types
none_source_type = komico_faq_it,komico_faq_lab,komico_voc
# Number of sources to include
source_count = 2
json_config_path = C:/Projects/01.Python/MAI-CHAT-PROD/llm-ollama-wrapper/config/prompts
llm_prompt_path = C:/Projects/01.Python/MAI-CHAT-PROD/llm-ollama-wrapper/config/prompts/mai-chat-prompt_ko.json
history_prompt_path = C:/Projects/01.Python/MAI-CHAT-PROD/llm-ollama-wrapper/config/prompts/mai-chat-history-prompt.json

[VOC]
voc_type = komico_voc
gw_doc_id_prefix_url = https://intermico.com/app/approval/document
check_gw_word_link = 전자결재 링크정보
check_gw_word = 전자결재 문서번호
check_block_line = ---------
gw_doc_id_link_url_pattern = (https://intermico.com/app/approval/document((?:\/[\w]*)))
gw_doc_id_link_correct_pattern = https://intermico.com/app/approval/document(?:\/\d{8})

# Web Search Configuration
[WEB_SEARCH]
# Enable web search functionality
use_flag = False
# Document addition type (0: Replace, 1: Combine)
document_add_type = 1
# Default search region
region = kr-kr
# Maximum number of search results
max_results = 5

# Chat History Configuration
[CHAT_HISTORY]
# Enable chat history storage
enabled = True

# Redis Configuration
[REDIS]
# Redis host
host = 10.50.1.26
# Redis port
port = 6379
# Redis password
password = AI-success!
# Redis database index
database = 0
# Enable TTL for keys
ttl_enabled = True
# TTL duration in seconds
ttl_time = 1800
# Get message count(-1이면 전체)
get_message_count = 20

[CACHE]
max_size = 200
chain_ttl = 3600
max_concurrent_tasks = 50

[NLP_MODELS]
sentence_model_path = C:/Projects/01.Python/MAI-CHAT-PROD/llm-ollama-wrapper/models/sentence-transformer/distiluse-base-multilingual-cased-v2
bert_model_path = C:/Projects/01.Python/MAI-CHAT-PROD/llm-ollama-wrapper/models/bert/bert-base-multilingual-cased