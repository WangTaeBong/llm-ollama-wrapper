# LOG Config
[LOG]
log_level=DEBUG
log_backtrace=True
log_diagnose=True
log_retention_days=30
log_path=./logs
log_filename=MAIChat_LLM.log

[SERVICE]
port=8000
reload=False

[LLM-MODEL]
llm_model_type=LLAMA3
llm_model_path=C:/Projects/01.Python/MAI-CHAT-PROD/llm-wrapper/models/llama3-8b-q4/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf
llm_model_verbose=True
#llm_model_n_gpu_layers=36
llm_model_n_gpu_layers=-1
llm_model_n_ctx=8192
#llm_model_n_ctx=512
llm_model_n_threads=8
llm_model_n_batch=8
lll_model_max_tokens=1024
#lll_model_max_tokens=512
llm_model_temperature=0.1
llm_model_top_p=0.9
llm_model_streaming=False
llm_model_f16_kv=True
llm_model_repeat_penalty=1.1
llm_model_mirostat_mode=2

[OLLAMA]
access_type=URL
model_name=Aicess_llama3.1-Korean-8B-Q8_0:latest
ollama_url=http://119.194.163.134:42434

[API]
retrival_api=http://172.16.0.27:5010/v1/retriever

[RETRIEVER]
filter_flag=False

[QUERY-FILTER]
enable=False
ko_jamo=False
arabia_num=False
wild_char=False

[LM-CHECK]
query_dict_config_path=C:/Projects/01.Python/MAI-CHAT-PROD/llm-wrapper/config/query_check_dict.json
query_lang_key=ko,en,jp,cn
query_dict_key=greetings,endings,farewells

[PROMPT]
faq_type=komico_faq
source_type=mico_hr,komico_hr,ceramics_hr,mico_lab,komico_lab,ceramics_lab,komico_voc
source_count=-1