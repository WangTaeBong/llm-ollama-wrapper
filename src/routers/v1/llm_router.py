import logging
from typing import Annotated

from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks
from starlette.responses import StreamingResponse

from src.schema.chat_req import ChatRequest
from src.schema.chat_res import ChatResponse
from src.services.chat.factory import ChatServiceFactory
from src.common.config_loader import ConfigLoader
from src.common.dependencies import get_validated_request, log_request

# Configure logger
logger = logging.getLogger(__name__)

# Load application settings from configuration
settings = ConfigLoader().get_settings()

# Router configuration
llm_router = APIRouter(prefix='/v1', tags=["LLM Chat"])


@llm_router.post(
    path="/chat",
    response_model=ChatResponse,
    summary="Process chat requests",
    description="Process user messages and generate responses using the LLM service.",
    response_description="Chat response generated by the LLM",
    status_code=status.HTTP_200_OK,
    responses={
        400: {"description": "Invalid request parameters or empty response"},
        500: {"description": "Internal server error"}
    }
)
async def chat(
        request: Annotated[ChatRequest, Depends(log_request)],
) -> ChatResponse:
    """
    Endpoint for processing chat requests.

    This endpoint handles the entire lifecycle of a chat request:
    1. Receives and validates the user's message
    2. Processes the message through the LLM service
    3. Returns the generated response or appropriate error

    Args:
        request (ChatRequest): The validated chat request data

    Returns:
        ChatResponse: The processed chat response from the LLM

    Raises:
        HTTPException(400): For validation failures or empty responses
        HTTPException(500): For unexpected server errors
    """
    try:
        logger.info(f"[{request.meta.session_id}] 채팅 요청 수신")

        # Get chat service instance via factory
        chat_service = await ChatServiceFactory.create_service(request)

        # Process the chat request
        response = await chat_service.process_chat()

        # Verify we received a valid response
        if not response:
            logger.warning("Empty response received from ChatService")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Chat service returned an empty response",
            )

        return response

    except ValueError as ve:
        # Handle validation errors
        error_msg = str(ve)
        logger.warning(f"Validation error in chat request: {error_msg}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=error_msg
        )

    except HTTPException:
        # Re-raise any existing HTTPExceptions
        raise

    except Exception as e:
        # Handle unexpected errors with a unique reference ID for tracing
        error_id = id(e)  # Generate unique ID for this error instance
        logger.exception(f"Unexpected error in chat endpoint [ID: {error_id}]: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Internal server error occurred. Reference ID: {error_id}",
        )


@llm_router.post(
    path="/chat/stream",
    response_class=StreamingResponse,
    summary="Stream chat responses",
    description="Incrementally stream chat responses to the client using SSE format",
    responses={
        400: {"description": "Invalid request parameters"},
        500: {"description": "Internal server error"}
    }
)
async def chat_stream(
        request: Annotated[ChatRequest, Depends(log_request)],
        background_tasks: BackgroundTasks
):
    """
    스트리밍 채팅 API 엔드포인트

    클라이언트에 점진적으로 응답을 스트리밍하는 채팅 API입니다.
    SSE(Server-Sent Events) 형식으로 응답을 전송합니다.

    Args:
        request: 채팅 요청 데이터
        background_tasks: 백그라운드 작업 실행기

    Returns:
        StreamingResponse: 스트리밍 응답 객체
    """
    logger.info(f"[{request.meta.session_id}] 스트리밍 채팅 요청 수신")

    # 설정에서 스트리밍 활성화 여부 확인
    if not settings.llm.steaming_enabled:
        logger.warning(f"[{request.meta.session_id}] 스트리밍이 설정에서 비활성화되어 있습니다.")

        # 오류 응답 스트림
        async def error_stream():
            error_msg = {"error": True, "text": "스트리밍이 서버 설정에서 비활성화되어 있습니다.", "finished": True}
            yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"

        return StreamingResponse(error_stream(), media_type="text/event-stream; charset=utf-8")

    # vLLM 백엔드 확인
    if settings.llm.llm_backend.lower() != "vllm":
        logger.warning(f"[{request.meta.session_id}] 스트리밍은 vLLM 백엔드에서만 지원됩니다.")

        # 오류 응답 스트림
        async def error_stream():
            error_msg = {"error": True, "text": "스트리밍은 vLLM 백엔드에서만 지원됩니다.", "finished": True}
            yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"

        return StreamingResponse(error_stream(), media_type="text/event-stream; charset=utf-8")

    try:
        # Get chat service instance via factory
        chat_service = await ChatServiceFactory.create_service(request)

        # 스트리밍 응답 처리
        response = await chat_service.stream_chat(background_tasks)

        # 중요: Content-Type 헤더에 charset=utf-8 추가
        response.headers["Content-Type"] = "text/event-stream; charset=utf-8"

        return response
    except Exception as e:
        logger.error(f"[{request.meta.session_id}] 스트리밍 처리 중 오류: {str(e)}", exc_info=True)

        # 오류 응답 스트림
        async def error_stream():
            # 중요: JSON 직렬화 시 ensure_ascii=False 설정
            import json
            error_msg = json.dumps(
                {"error": True, "text": f"처리 중 오류가 발생했습니다: {str(e)}", "finished": True},
                ensure_ascii=False
            )
            yield f"data: {error_msg}\n\n"

        return StreamingResponse(
            error_stream(),
            media_type="text/event-stream; charset=utf-8"  # charset 명시
        )
