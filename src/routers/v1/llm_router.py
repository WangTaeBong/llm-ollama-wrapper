import json
import logging

from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks
from starlette.responses import StreamingResponse

from src.schema.chat_req import ChatRequest
from src.schema.chat_res import ChatResponse
from src.services.llm_ollama_process import ChatService
from src.common.config_loader import ConfigLoader

# Configure logger
logger = logging.getLogger(__name__)

# Load application settings from configuration
settings = ConfigLoader().get_settings()

# Router configuration
llm_router = APIRouter(prefix='/v1', tags=["LLM Chat"])


async def log_request(request: ChatRequest) -> ChatRequest:
    """
    Dependency function for logging incoming requests.

    Logs the content of chat requests in debug mode with sensitive information masked.
    This function maintains a balance between comprehensive logging for debugging
    purposes and security/privacy concerns.

    Args:
        request (ChatRequest): The original chat request object

    Returns:
        ChatRequest: The unmodified original request object (passed through)
    """
    # Create a deep copy of the request to avoid modifying the original
    safe_request = request.copy(deep=True)

    # Mask sensitive information (if present)
    if hasattr(safe_request, 'api_key') and getattr(safe_request, 'api_key', None):
        setattr(safe_request, 'api_key', "********")  # Mask API key

    # Log basic request info at INFO level
    logger.info(f"Request[{safe_request.meta.rag_sys_info}/{safe_request.meta.session_id}] {safe_request.chat.user}")

    # Log detailed request data at DEBUG level
    logger.debug(f"Received chat request: {safe_request}")

    return request


def get_chat_service(request: ChatRequest) -> ChatService:
    """
    Dependency function for creating a ChatService instance.

    Initializes and returns a new ChatService instance configured with the
    provided request parameters. This dependency injection approach simplifies
    testing and allows for better separation of concerns.

    Args:
        request (ChatRequest): The validated chat request object

    Returns:
        ChatService: An initialized ChatService instance ready to process the request
    """
    return ChatService(request)


@llm_router.post(
    path="/chat",
    response_model=ChatResponse,
    summary="Process chat requests",
    description="Process user messages and generate responses using the LLM service.",
    response_description="Chat response generated by the LLM",
    status_code=status.HTTP_200_OK,
    responses={
        400: {"description": "Invalid request parameters or empty response"},
        500: {"description": "Internal server error"}
    }
)
async def chat(
        request: ChatRequest = Depends(log_request),
        chat_service: ChatService = Depends(get_chat_service),
) -> ChatResponse:
    """
    Endpoint for processing chat requests.

    This endpoint handles the entire lifecycle of a chat request:
    1. Receives and validates the user's message
    2. Processes the message through the LLM service
    3. Returns the generated response or appropriate error

    The function uses FastAPI's dependency injection system to:
    - Log the incoming request (via log_request)
    - Initialize the chat service (via get_chat_service)

    Args:
        request (ChatRequest): The validated chat request data
        chat_service (ChatService): The initialized chat service instance

    Returns:
        ChatResponse: The processed chat response from the LLM

    Raises:
        HTTPException(400): For validation failures or empty responses
        HTTPException(500): For unexpected server errors
    """
    try:
        logger.info(f"[{request.meta.session_id}] 채팅 요청 수신")

        # Process the chat request
        response = await chat_service.process_chat()

        # Verify we received a valid response
        if not response:
            logger.warning("Empty response received from ChatService")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Chat service returned an empty response",
            )

        return response

    except ValueError as ve:
        # Handle validation errors
        error_msg = str(ve)
        logger.warning(f"Validation error in chat request: {error_msg}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=error_msg
        )

    except HTTPException:
        # Re-raise any existing HTTPExceptions
        raise

    except Exception as e:
        # Handle unexpected errors with a unique reference ID for tracing
        error_id = id(e)  # Generate unique ID for this error instance
        logger.exception(f"Unexpected error in chat endpoint [ID: {error_id}]: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Internal server error occurred. Reference ID: {error_id}",
        )


@llm_router.post(path="/chat/stream", response_class=StreamingResponse)
async def chat_stream(request: ChatRequest, background_tasks: BackgroundTasks):
    """
    스트리밍 채팅 API 엔드포인트

    클라이언트에 점진적으로 응답을 스트리밍하는 채팅 API입니다.
    SSE(Server-Sent Events) 형식으로 응답을 전송합니다.

    Args:
        request: 채팅 요청 데이터
        background_tasks: 백그라운드 작업 실행기

    Returns:
        StreamingResponse: 스트리밍 응답 객체
    """
    logger.info(f"[{request.meta.session_id}] 스트리밍 채팅 요청 수신")

    # 설정에서 스트리밍 활성화 여부 확인
    if not settings.llm.steaming_enabled:
        logger.warning(f"[{request.meta.session_id}] 스트리밍이 설정에서 비활성화되어 있습니다.")

        # 오류 응답 스트림
        async def error_stream():
            error_msg = {"error": True, "text": "스트리밍이 서버 설정에서 비활성화되어 있습니다.", "finished": True}
            yield f"data: {json.dumps(error_msg)}\n\n"

        return StreamingResponse(error_stream(), media_type="text/event-stream")

    # vLLM 백엔드 확인
    if settings.llm.llm_backend.lower() != "vllm":
        logger.warning(f"[{request.meta.session_id}] 스트리밍은 vLLM 백엔드에서만 지원됩니다.")

        # 오류 응답 스트림
        async def error_stream():
            error_msg = {"error": True, "text": "스트리밍은 vLLM 백엔드에서만 지원됩니다.", "finished": True}
            yield f"data: {json.dumps(error_msg)}\n\n"

        return StreamingResponse(error_stream(), media_type="text/event-stream")

    # 채팅 서비스 초기화
    chat_service = ChatService(request)

    try:
        # 스트리밍 응답 처리
        response = await chat_service.stream_chat(background_tasks)

        # 중요: Content-Type 헤더에 charset=utf-8 추가
        response.headers["Content-Type"] = "text/event-stream; charset=utf-8"

        return response
    except Exception as e:
        logger.error(f"[{request.meta.session_id}] 스트리밍 처리 중 오류: {str(e)}", exc_info=True)

        # 오류 응답 스트림
        async def error_stream():
            # 중요: JSON 직렬화 시 ensure_ascii=False 설정
            error_msg = json.dumps(
                {"error": True, "text": f"처리 중 오류가 발생했습니다: {str(e)}", "finished": True},
                ensure_ascii=False
            )
            yield f"data: {error_msg}\n\n"

        return StreamingResponse(
            error_stream(),
            media_type="text/event-stream; charset=utf-8"  # charset 명시
        )
